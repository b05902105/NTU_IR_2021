{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "069c3495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter, defaultdict\n",
    "import _pickle as pk\n",
    "from time import time\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "def read_query(filepath):\n",
    "    tree = ET.parse(filepath)\n",
    "    questions = []\n",
    "    for p in tree.findall('topic'):\n",
    "        questions.append([x.text.strip() for x in p.findall('*')])\n",
    "    return questions\n",
    "\n",
    "def query_processing(questions):\n",
    "    questions = [x[-1][:-1].split('„ÄÅ') for x in questions]\n",
    "    res = []\n",
    "    for q in questions:\n",
    "        uni_qspace = set()\n",
    "        bi_qspace = set()\n",
    "        for words in q:\n",
    "            uni_qspace |= set(list(words))\n",
    "            bi_qspace |= set([words[i:i+2] for i in range(len(words)-1)])\n",
    "        res.append((list(uni_qspace), list(bi_qspace)))\n",
    "    return res\n",
    "\n",
    "def open_file(path):\n",
    "    tree = ET.parse(path.strip())\n",
    "    id_ = tree.find('.//id').text\n",
    "    text = ''.join([x.text.strip() for x in tree.findall('.//p')])\n",
    "    return id_, text\n",
    "    \n",
    "def BM25_score(d, doc_id, query, k1=1.2, b=0.75):\n",
    "    score = 0\n",
    "    for term in query:\n",
    "        tf = d.docs[doc_id][term]\n",
    "        doc_len = d.docs_length[doc_id]\n",
    "        \n",
    "        nqi = d.df[term]\n",
    "        idf = np.log((d.num_docs - nqi + 0.5)/(nqi + 0.5) + 1)\n",
    "        \n",
    "        score += idf * (tf*(k1 + 1))/(tf + k1*(1 - b + b*(doc_len / d.avg_length)))\n",
    "    return score\n",
    "\n",
    "def get_result_list(q_id, filelist, res):\n",
    "    return [q_id, ' '.join([open_file(filelist[i])[0].lower() for i in res])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f542fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self.filelist = None\n",
    "        self.vocab = None\n",
    "\n",
    "        self.inverted_file = defaultdict(list) # {'a': [(doc_id, cnt), (doc_id, cnt)], 'b': []}\n",
    "        self.words = Counter() # {'a': 100, 'b': 20}\n",
    "        self.docs_length = list() # [10, 20, 30]\n",
    "        self.avg_length = 0\n",
    "        self.docs = list() # [{'a': 10, 'b': 20}, {'a': 5, 'b': 10}, {'c': 30}]\n",
    "        self.num_docs = 0 # the number of documents\n",
    "        self.df = Counter() # A word 'a' appears in how many documents.\n",
    "        \n",
    "        self.start_time = 0\n",
    "    def parsing(self, path):\n",
    "        with open(join(path, 'model/file-list'), 'r') as f:\n",
    "            self.filelist = [join(path, 'CIRB010', s.strip()) for s in f.readlines()]\n",
    "            \n",
    "        self.num_docs = len(self.filelist)\n",
    "        self.docs_length = [0]*self.num_docs\n",
    "        self.docs = [Counter() for i in range(self.num_docs)]\n",
    "        \n",
    "        with open(join(path, 'model/vocab.all'), 'r') as f:\n",
    "            self.vocab = f.read().split()\n",
    "        \n",
    "        with open(join(path, 'model/inverted-file'), 'r') as f:\n",
    "            self.start_time = time()\n",
    "            lines = f.readlines()\n",
    "            i, end = 0, len(lines)\n",
    "            while i < end:\n",
    "                if i % 10 == 0:\n",
    "                    print('Processing .... %06.2f%%, total time: %06.2f sec.' % (100*(i+1)/end, time() - self.start_time), end='\\r')\n",
    "                st, nd, cnt = [int(x) for x in lines[i].split()]\n",
    "                \n",
    "                if re.match('\\w', self.vocab[st]):\n",
    "                    w = self.vocab[st] if nd < 0 else self.vocab[st] + self.vocab[nd]\n",
    "                    self.df[w] += cnt\n",
    "\n",
    "                    for line in lines[i+1:i+cnt+1]:\n",
    "                        doc_id, w_cnt = tuple([int(x) for x in line.split()])\n",
    "                        self.words[w] += w_cnt\n",
    "                        self.inverted_file[w].append((doc_id, w_cnt))\n",
    "                        if nd < 0:\n",
    "                            self.docs_length[doc_id] += w_cnt\n",
    "                        self.docs[doc_id][w] += w_cnt\n",
    "                i += cnt + 1\n",
    "        self.avg_length = np.mean(d.docs_length)\n",
    "    def save(self, name):\n",
    "        with open(name, 'wb') as pf:\n",
    "            pk.dump(self.__dict__, pf)\n",
    "    \n",
    "    def load(self, name):\n",
    "        with open(name, 'rb') as pf:\n",
    "            self.__dict__ = pk.load(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dbc2371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing .... 100.00%, total time: 244.66 sec.\n",
      "Processing 30 / 30, total time: 197.26 sec.\r"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data_path = '/tmp2/r09922104/ir'\n",
    "    train_path = join(data_path, 'queries/query-train.xml')\n",
    "    test_path = join(data_path, 'queries/query-test.xml')\n",
    "    \n",
    "    save = False\n",
    "    \n",
    "    d = Dataset()\n",
    "    if save:\n",
    "        d.load('/tmp2/r09922104/data/data.pkl')\n",
    "    else:\n",
    "        d.parsing(data_path)\n",
    "        d.save('/tmp2/r09922104/data/data.pkl')\n",
    "\n",
    "    questions = read_query(train_path) + read_query(test_path)\n",
    "    queries = query_processing(questions)\n",
    "    \n",
    "    result = []\n",
    "    start = time()\n",
    "    print()\n",
    "    \n",
    "    for _, query in enumerate(queries):\n",
    "        print('Processing %02d / %02d, total time: %06.2f sec.' % (_+1, len(queries), time() - start), end='\\r')\n",
    "        uni, bi = query\n",
    "        candidate = set()\n",
    "        for word in bi:\n",
    "            candidate |= set([d[0] for d in d.inverted_file[word]])\n",
    "        candidate = list(candidate)\n",
    "\n",
    "        scores = [BM25_score(d, doc_id, uni+bi) for doc_id in candidate]\n",
    "        rank = np.argsort(scores)\n",
    "        res = [candidate[i] for i in rank[-100:][::-1]]\n",
    "        result.append(get_result_list('%03d' % (_+1), d.filelist, res))\n",
    "        \n",
    "    pd.DataFrame(result).to_csv('out.csv', header=['query_id','retrieved_docs'], index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir",
   "language": "python",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
