{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "581dc0e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter, defaultdict\n",
    "import _pickle as pk\n",
    "from time import time\n",
    "import jieba\n",
    "\n",
    "from queue import Queue\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "import threading\n",
    "\n",
    "def read_query(filepath):\n",
    "    tree = ET.parse(filepath)\n",
    "    questions = []\n",
    "    for p in tree.findall('topic'):\n",
    "        questions.append([x.text.strip() for x in p.findall('*')])\n",
    "    return questions\n",
    "\n",
    "def dump_files(path, output_file='filelist.txt'):\n",
    "    def foo(path, f):\n",
    "        if os.path.isdir(path):\n",
    "            for p in sorted(os.listdir(path)):\n",
    "                foo(join(path, p), f)\n",
    "        else:\n",
    "            f.write(path + '\\n')\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        foo(path, f)\n",
    "        \n",
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self.filelist = None\n",
    "\n",
    "        self.inverted_file = defaultdict(list) # {'a': [(doc_id, cnt), (doc_id, cnt)], 'b': []}\n",
    "        self.words = Counter() # {'a': 100, 'b': 20}\n",
    "#         self.docs_length = list() # [10, 20, 30]\n",
    "#         self.docs = list() # [{'a': 10, 'b': 20}, {'a': 5, 'b': 10}, {'c': 30}]\n",
    "        self.num_docs = 0 # the number of documents\n",
    "#         self.num_words = 0 # the number of words\n",
    "        self.df = Counter() # A word 'a' appears in how many documents.\n",
    "\n",
    "        self.vocab = None\n",
    "    def threading_func(self, thd_idx, lines, i, end, q):\n",
    "        tmp_df = Counter()\n",
    "        tmp_words = Counter()\n",
    "        tmp_inverted_file = defaultdict(list)\n",
    "        while i < end:\n",
    "            if thd_idx == 0:\n",
    "                print('Processing .... %08d / %08d, total time: %06.2f sec.' % ((i+1), end, time() - self.start_time), end='\\r')\n",
    "            st, nd, cnt = [int(x) for x in lines[i].split()]\n",
    "            w = self.vocab[st] if nd < 0 else self.vocab[st] + self.vocab[nd]\n",
    "            tmp_df[w] += cnt\n",
    "            \n",
    "            for line in lines[i+1:i+cnt+1]:\n",
    "                doc_id, w_cnt = tuple([int(x) for x in line.split()])\n",
    "                tmp_words[w] += w_cnt\n",
    "                tmp_inverted_file[w].append((doc_id, w_cnt))\n",
    "            i += cnt + 1\n",
    "        q.put((tmp_df, tmp_words, tmp_inverted_file))\n",
    "            \n",
    "    def read_from_file(self, path='./data/model'):\n",
    "        with open(join(path, 'file-list'), 'r') as f:\n",
    "            self.filelist = f.readlines()\n",
    "        self.num_docs = len(self.filelist)\n",
    "        \n",
    "        with open(join(path, 'vocab.all'), 'r') as f:\n",
    "            self.vocab = f.read().split()\n",
    "        \n",
    "        with open(join(path, 'inverted-file'), 'r') as f:\n",
    "            self.start_time = time()\n",
    "            lines = f.readlines()\n",
    "            vocab_idx = []\n",
    "            start_idx = np.linspace(0, len(lines), 9).astype(int)\n",
    "            start_idx = [0, 4665362, 9330749, 13995841, 18660933, 23327811, 27991067, 32655803, len(lines)]\n",
    "            start_idx = start_idx[::2]\n",
    "#             def threading_func(thd_idx, lines, start, end, q):\n",
    "#                 ret = []\n",
    "#                 while start < end:\n",
    "#                     st, nd, cnt = [int(x) for x in lines[start].split()]\n",
    "#                     ret.append(start)\n",
    "#                     start += cnt + 1\n",
    "#                 q.put(ret) \n",
    "            \n",
    "            q = Queue()\n",
    "            thd_list = [threading.Thread(target=self.threading_func, args=(i, lines, start_idx[i], start_idx[i+1], q)) for i in range(len(start_idx)-1)]\n",
    "            \n",
    "            for thd in thd_list:\n",
    "                thd.start()\n",
    "            for thd in thd_list:\n",
    "                thd.join()\n",
    "                \n",
    "            for _ in range(len(thd_list)):\n",
    "                df, words, inverted_file = q.get()\n",
    "                self.df += df\n",
    "                self.words += words\n",
    "                for k, v in inverted_file.items():\n",
    "                    self.inverted_file[k] += v\n",
    "            print('\\ntotal time: %06.2f sec.' % (time() - self.start_time))\n",
    "#             for _ in range(len(thd_list)):\n",
    "#                 vocab_idx += q.get()\n",
    "#             i = 0\n",
    "#             while i < self.num_docs:\n",
    "#                 print('Processing .... %06.2f %%, total time: %06.2f sec.' % (100*(i+1)/len(lines), time() - start), end='\\r')\n",
    "#                 st, nd, cnt = [int(x) for x in lines[i].split()]\n",
    "#                 vocab_idx.append(i)\n",
    "#                 i += cnt + 1\n",
    "#         return vocab_idx\n",
    "#         with open(join(path, 'inverted-file'), 'r') as f:\n",
    "#             start = time()\n",
    "#             i = 0\n",
    "#             lines = f.readlines()\n",
    "#             self.num_docs = len(lines)\n",
    "#             self.docs_length = [0]*self.num_docs\n",
    "#             while i < self.num_docs:\n",
    "#                 print('Processing .... %06.2f %%, total time: %06.2f sec.' % (100*(i+1)/len(lines), time() - start), end='\\r')\n",
    "#                 st, nd, cnt = [int(x) for x in lines[i].split()]\n",
    "#                 w = vocab[st] if nd < 0 else vocab[st] + vocab[nd]\n",
    "                \n",
    "#                 self.df[w] += cnt\n",
    "#                 for line in lines[i+1:i+cnt+1]:\n",
    "#                     doc_id, w_cnt = tuple([int(x) for x in line.split()])\n",
    "#                     self.words[w] += w_cnt\n",
    "#                     try:\n",
    "#                         self.inverted_file[w].append((doc_id, w_cnt))\n",
    "#                     except:\n",
    "#                         self.inverted_file[w] = [(doc_id, w_cnt)]\n",
    "                \n",
    "#                 i += cnt + 1\n",
    "                \n",
    "#     def read_data(self, filelist_path):\n",
    "#         with open(filelist_path, 'r') as f:\n",
    "#             self.filelist = f.readlines()\n",
    "#         self.num_docs = len(self.filelist)\n",
    "#         start = time()\n",
    "#         for i, file in enumerate(self.filelist):\n",
    "#             print('reading file.... %04d/%04d, total time : %06.2f sec.' % (i+1, len(lines), time() - start), end='\\r')\n",
    "#             tree = ET.parse(file.strip())\n",
    "#             if tree.find('.//title'):\n",
    "#                 title = tree.find('.//title').text.strip()\n",
    "\n",
    "#             text = ''.join([x.text.strip() for x in tree.findall('.//p')])\n",
    "#             chinese_text = re.findall(r\"[\\u4e00-\\u9fa5']+\", text)\n",
    "#             eng_text = re.findall(r\"[A-aZ-z']+\", text)\n",
    "\n",
    "#             doc_dict = Counter()\n",
    "\n",
    "#             # Chinese cut word\n",
    "#             for sentence in chinese_text:\n",
    "#                 cut_word = jieba.lcut(sentence)\n",
    "#                 for w in cut_word:\n",
    "#                     if len(w) > 1:\n",
    "#                         cnt = Counter([w] + list(w))\n",
    "#                     else:\n",
    "#                         cnt = Counter(list(w))\n",
    "#                     doc_dict += cnt\n",
    "#             # bi_text = [x[i:i+2] for x in chinese_text for i in range(len(x)-1)]\n",
    "\n",
    "#             doc_dict += Counter(eng_text)\n",
    "#             self.words += doc_dict\n",
    "#             self.docs.append(doc_dict)\n",
    "#             self.docs_length.append(len(chinese_text)+len(eng_text))\n",
    "#             self.df += Counter(list(doc_dict.keys()))\n",
    "\n",
    "#             for k, v in doc_dict.items():\n",
    "#                 try:\n",
    "#                     self.inverted_file[k].append((i, v))\n",
    "#                 except:\n",
    "#                     self.inverted_file[k] = [(i, v)]\n",
    "    def save(self, name):\n",
    "        with open(name, 'wb') as pf:\n",
    "            pk.dump(self.__dict__, pf)\n",
    "    \n",
    "    def load(self, name):\n",
    "        with open(name, 'rb') as pf:\n",
    "            self.__dict__ = pk.load(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3419a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing .... 09327668 / 09330749, total time: 090.70 sec.\n",
      "total time: 097.48 sec.\n"
     ]
    }
   ],
   "source": [
    "d = Dataset()\n",
    "res = d.read_from_file()\n",
    "# d.read_data('filelist.txt')\n",
    "# d.load('mydata.pkl')\n",
    "d.save('data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a43eec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/queries/query-train.xml'\n",
    "questions = read_query(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir",
   "language": "python",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
